{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2: OOP with application to scVI\n",
    "This exercise will help you to identify how OOP has been used in scvi codes to build Deep Generative modeling for Single Cell Transcriptonomics. You can find more about the basic usage from __[scVI_Basic_Usage](https://github.com/YosefLab/scVI/blob/master/tests/notebooks/basic_tutorial.ipynb)__ \n",
    "<br> In the following exercise we will work with some extracts of codes from scvi notebooks, and try to answer the questions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2.1\n",
    "The following is an excerpt from the __Trainer__ class under inference subfolder. The whole file is available under: __[scVI_trainer.py](https://github.com/YosefLab/scVI/blob/master/scvi/inference/trainer.py)__ :\n",
    "<br> \\[N.B. Do not worry if you still donot understand everything about the code at this point as we will talk about inference and variational autoencoders later on in this course. Also some parts have been omitted for simplicity\\] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer:\n",
    "    r\"\"\"The abstract Trainer class for training a PyTorch model and monitoring its statistics. It should be\n",
    "    inherited at least with a .loss() function to be optimized in the training loop.\n",
    "    Args:\n",
    "        :model: A model instance from class ``VAE``, ``VAEC``, ``SCANVI``\n",
    "        :gene_dataset: A gene_dataset instance like ``CortexDataset()``\n",
    "        :use_cuda: Default: ``True``.\n",
    "        :metrics_to_monitor: A list of the metrics to monitor. If not specified, will use the\n",
    "            ``default_metrics_to_monitor`` as specified in each . Default: ``None``.\n",
    "        :benchmark: if True, prevents statistics computation in the training. Default: ``False``.\n",
    "        :verbose: If statistics should be displayed along training. Default: ``None``.\n",
    "        :frequency: The frequency at which to keep track of statistics. Default: ``None``.\n",
    "        :early_stopping_metric: The statistics on which to perform early stopping. Default: ``None``.\n",
    "        :save_best_state_metric:  The statistics on which we keep the network weights achieving the best store, and\n",
    "            restore them at the end of training. Default: ``None``.\n",
    "        :on: The data_loader name reference for the ``early_stopping_metric`` and ``save_best_state_metric``, that\n",
    "            should be specified if any of them is. Default: ``None``.\n",
    "    \"\"\"\n",
    "    default_metrics_to_monitor = []\n",
    "\n",
    "    def __init__(self, model, gene_dataset, use_cuda=True, metrics_to_monitor=None, benchmark=False,\n",
    "                 verbose=False, frequency=None, weight_decay=1e-6, early_stopping_kwargs=dict(),\n",
    "                 data_loader_kwargs=dict()):\n",
    "\n",
    "        self.model = model\n",
    "        self.gene_dataset = gene_dataset\n",
    "\n",
    "        self.data_loader_kwargs = {\n",
    "            \"batch_size\": 128,\n",
    "            \"pin_memory\": use_cuda\n",
    "        }\n",
    "        self.data_loader_kwargs.update(data_loader_kwargs)\n",
    "\n",
    "        self.weight_decay = weight_decay\n",
    "        self.benchmark = benchmark\n",
    "        self.epoch = -1  # epoch = self.epoch + 1 in compute metrics\n",
    "        self.training_time = 0\n",
    "        self.early_stopping = EarlyStopping(**early_stopping_kwargs)\n",
    "    \n",
    "    def __getattr__(self, name):\n",
    "        if '_posteriors' in self.__dict__:\n",
    "            _posteriors = self.__dict__['_posteriors']\n",
    "            if name.strip('_') in _posteriors:\n",
    "                return _posteriors[name.strip('_')]\n",
    "        return object.__getattribute__(self, name)\n",
    "\n",
    "    def __delattr__(self, name):\n",
    "        if name.strip('_') in self._posteriors:\n",
    "            del self._posteriors[name.strip('_')]\n",
    "        else:\n",
    "            object.__delattr__(self, name)\n",
    "            \n",
    "\n",
    "    def register_posterior(self, name, value):\n",
    "        name = name.strip('_')\n",
    "        self._posteriors[name] = value\n",
    "\n",
    "    def corrupt_posteriors(self, rate=0.1, corruption=\"uniform\", update_corruption=True):\n",
    "        if not hasattr(self.gene_dataset, 'corrupted') and update_corruption:\n",
    "            self.gene_dataset.corrupt(rate=rate, corruption=corruption)\n",
    "        for name, posterior in self._posteriors.items():\n",
    "            self.register_posterior(name, posterior.corrupted())\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on your previous knowledge of OOP and looking at the code above, answer the following questions:\n",
    "1. What are the arguments with default values stated in the __Trainer__ class i.e. name the arguments which are not necessary to include while instantiating the __Trainer__ class?\n",
    "2. Can you name at least 2 model names the user can use as arguments while instantiating the class __Trainer__? e.g: train_object=Trainer(model=?,gene_dataset,...)\n",
    "3. Can you name two arguments where you can use variable length of the arguments while instantiating the __Trainer__ class?\n",
    "4. List all the methods that you can see under __Trainer__ class. Which of these are Dunder methods?\n",
    "5. We want to be able to call the __corrupt\\_posteriors__ method as an attribute of the __Trainer__ class. Modify the above code to enable this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Exercise 4.2\n",
    "The following is an excerpt from the __Inference.py__ file under inference subfolder. The whole file is available under: __[scVI_inference.py](https://github.com/YosefLab/scVI/blob/master/scvi/inference/inference.py)__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UnsupervisedTrainer(Trainer):\n",
    "    r\"\"\"The VariationalInference class for the unsupervised training of an autoencoder.\n",
    "    Args:\n",
    "        :model: A model instance from class ``VAE``, ``VAEC``, ``SCANVI``\n",
    "        :gene_dataset: A gene_dataset instance like ``CortexDataset()``\n",
    "        :train_size: The train size, either a float between 0 and 1 or and integer for the number of training samples\n",
    "         to use Default: ``0.8``.\n",
    "        :\\*\\*kwargs: Other keywords arguments from the general Trainer class.\n",
    "    Examples:\n",
    "        >>> gene_dataset = CortexDataset()\n",
    "        >>> vae = VAE(gene_dataset.nb_genes, n_batch=gene_dataset.n_batches * False,\n",
    "        ... n_labels=gene_dataset.n_labels)\n",
    "        >>> infer = VariationalInference(gene_dataset, vae, train_size=0.5)\n",
    "        >>> infer.train(n_epochs=20, lr=1e-3)\n",
    "    \"\"\"\n",
    "    default_metrics_to_monitor = ['ll']\n",
    "\n",
    "    def __init__(self, model, gene_dataset, train_size=0.8, test_size=None, kl=None, **kwargs):\n",
    "        super().__init__(model, gene_dataset, **kwargs)\n",
    "        self.kl = kl\n",
    "        if type(self) is UnsupervisedTrainer:\n",
    "            self.train_set, self.test_set = self.train_test(model, gene_dataset, train_size, test_size)\n",
    "            self.train_set.to_monitor = ['ll']\n",
    "            self.test_set.to_monitor = ['ll']\n",
    "\n",
    "    @property\n",
    "    def posteriors_loop(self):\n",
    "        return ['train_set']\n",
    "\n",
    "\n",
    "class AdapterTrainer(UnsupervisedTrainer):\n",
    "    def __init__(self, model, gene_dataset, posterior_test, frequency=5):\n",
    "        super().__init__(model, gene_dataset, frequency=frequency)\n",
    "        self.test_set = posterior_test\n",
    "        self.test_set.to_monitor = ['ll']\n",
    "        self.params = list(self.model.z_encoder.parameters()) + list(self.model.l_encoder.parameters())\n",
    "\n",
    "    @property\n",
    "    def posteriors_loop(self):\n",
    "        return ['test_set']\n",
    "\n",
    "    def train(self, n_path=10, n_epochs=50, **kwargs):\n",
    "        for i in range(n_path):\n",
    "            # Re-initialize to create new path\n",
    "            self.model.z_encoder.load_state_dict(self.z_encoder_state)\n",
    "            self.model.l_encoder.load_state_dict(self.l_encoder_state)\n",
    "            super().train(n_epochs, params=self.params, **kwargs)\n",
    "\n",
    "        return min(self.history[\"ll_test_set\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer the following questions:\n",
    "1. Which class does __UnsupervisedTrainer__ inherit from?\n",
    "2. If we print the AdapterTrainer.\\_\\_mro\\_\\_, what order of inheritance would you see?\n",
    "3. Look at the __posteriors\\_loop__ method for the class AdapterTrainer. Modify the code such that now we donot want this to be the attribute of the class, but rather as a method which takes in argument _set_ and returns this value and default it to ['test_set']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1_oop_and_decorators_exercises.ipynb   img\r\n",
      " 2_oop_in_machine_learning.ipynb        Only_OOP_slides.pdf\r\n",
      "'Basics of OOP with Python_v2 .ipynb'  'OOP Solutions.ipynb'\r\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/data/pCloudDrive/W/dl_projects_combined/Modules/python-for-machine-learning-course/content/modules/1_OOP\r\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
